{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91260bd-6089-41f0-ae99-6b525c52cb90",
   "metadata": {},
   "source": [
    "## Sample masking for Flash attention 2.\n",
    "\n",
    "Llama3 architecture exploits self-attention masking for different samples, as shown in https://arxiv.org/pdf/2407.21783 3.2 Model Architecture ```We use an attention mask that prevents self-attention between different documents within the same sequence. ```.\n",
    "\n",
    "It is easy to implement it for common self-attention (e.g., `torch.matmul`), while it needs several tricks to make the sample-level masking compatible with [flash-attention-v2](https://github.com/Dao-AILab/flash-attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec842d-434e-47e6-b6bf-e0d79401207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4c5d2-2878-4bfb-83ce-2cfe1ddfe1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "## Suppose there are three samples, they are tokenized into token ids.\n",
    "'''\n",
    "tokenizer('Hello world!').input_ids => [9906, 1917, 0]\n",
    "tokenizer('Nice to meet you!').input_ids => [46078, 311, 3449, 499, 0]\n",
    "tokenizer('What\\' you name?').input_ids => [3923, 6, 499, 836, 30]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2e1e9-3dd9-4c9f-a012-70157c260c82",
   "metadata": {},
   "source": [
    "Suppose there are two training sequences:\n",
    "1. 'Hello world!<|end_of_text|>Nice to meet you!<|end_of_text|>What\\' you name?<|end_of_text|>' => \\[9906, 1917, 0, 128001, 46078, 311, 3449, 499, 0, 128001, 3923, 6, 499, 836, 30, 128001\\]\n",
    "2. 'What\\' you name?<|end_of_text|>Nice to meet you!<|end_of_text|>Hello world!<|end_of_text|>' => \\[3923, 6, 499, 836, 30, 128001, 46078, 311, 3449, 499, 0, 128001, 9906, 1917, 0, 128001\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece270e-f393-463c-9c76-e3581fd92fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## <|end_of_text|>\n",
    "eos_token_id = 128001\n",
    "\n",
    "tokens = torch.tensor(\n",
    "    [\n",
    "        [9906, 1917, 0, 128001, 46078, 311, 3449, 499, 0, 128001, 3923, 6, 499, 836, 30, 128001],\n",
    "        [3923, 6, 499, 836, 30, 128001, 46078, 311, 3449, 499, 0, 128001, 9906, 1917, 0, 128001],\n",
    "    ],\n",
    "    dtype=torch.long)\n",
    "\n",
    "def to_device(batch, device):\n",
    "    output = {}\n",
    "    for k, v in batch.items():\n",
    "        try:\n",
    "            output[k] = v.to(device)\n",
    "        except:\n",
    "            output[k] = v\n",
    "    return output\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "batch = {\"input_ids\": tokens, 'labels': tokens}\n",
    "batch = to_device(batch, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e98fc5-51d1-4897-938c-13ab1faef3c4",
   "metadata": {},
   "source": [
    "### Option 1: No masking between different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463c67b-90cb-4e94-92ec-1dac6e2e87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16, use_flash_attention_2=True)\n",
    "\n",
    "outputs = model(**batch, use_cache=False)\n",
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348e4c0-c32f-45cd-a4f0-df0060067e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14de07bd-7141-44d1-b6b4-47ad99c0e810",
   "metadata": {},
   "source": [
    "### Option 2: Do masking between different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13bd64-788e-4b94-a352-be08cb5435a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from einops import rearrange\n",
    "from flash_attn.bert_padding import unpad_input, pad_input, index_first_axis\n",
    "\n",
    "def _get_unpad_data_for_concatenated_sequences(attention_mask_in_length):\n",
    "    \"\"\"\n",
    "    Supports concatenating short samples in one sequence. The attention_mask_in_length is utilized to mask other short samples. It helps efficient training of variant lengths-based samples (e.g., the supervised fine-tuning task in large language model).\n",
    "    The motivation for this function is explained [here](https://github.com/Dao-AILab/flash-attention/issues/432#issuecomment-1668822286).\n",
    "\n",
    "    For example, if batch = 3 and seqlen = 6, the attention_mask_in_length is:\n",
    "        ```\n",
    "        [\n",
    "          [2, 3, 0, 0, 0, 0],\n",
    "          [3, 2, 0, 0, 0, 0],\n",
    "          [6, 0, 0, 0, 0, 0]\n",
    "        ]\n",
    "        ```\n",
    "    , which refers to the 3D-attention mask:\n",
    "        ```\n",
    "        [\n",
    "          [\n",
    "            [1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 0, 0],\n",
    "            [0, 0, 1, 1, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 1]\n",
    "          ],\n",
    "          [\n",
    "            [1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 1]\n",
    "          ],\n",
    "          [\n",
    "            [1, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 0],\n",
    "            [1, 1, 1, 1, 1, 1]\n",
    "          ]\n",
    "        ]\n",
    "        ```.\n",
    "\n",
    "    Arguments:\n",
    "        attention_mask_in_length: (batch, seqlen), int, a nonzero number (e.g., 1, 2, 3, etc.) means length of concatenated sequence in b-th batch, and 0 means none.\n",
    "    Return:\n",
    "        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n",
    "        max_seqlen_in_batch: int\n",
    "    \"\"\"\n",
    "    length = attention_mask_in_length.sum(dim=-1)\n",
    "    seqlen = attention_mask_in_length.size(-1)\n",
    "    attention_mask_2d = torch.arange(seqlen, device=length.device, dtype=length.dtype).expand(len(length), seqlen) < length.unsqueeze(1)\n",
    "    real_indices_idx = torch.nonzero(attention_mask_in_length.flatten(), as_tuple=False).flatten()\n",
    "    seqlens_in_batch = attention_mask_in_length.flatten()[real_indices_idx]\n",
    "    indices = torch.nonzero(attention_mask_2d.flatten(), as_tuple=False).flatten()\n",
    "    max_seqlen_in_batch = seqlens_in_batch.max().item()\n",
    "    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0))\n",
    "    return (\n",
    "        indices,\n",
    "        cu_seqlens,\n",
    "        max_seqlen_in_batch,\n",
    "    )\n",
    "\n",
    "def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n",
    "    #indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n",
    "    indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data_for_concatenated_sequences(attention_mask)\n",
    "    \n",
    "    batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n",
    "\n",
    "    key_layer = index_first_axis(\n",
    "        key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    "    )\n",
    "    value_layer = index_first_axis(\n",
    "        value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    "    )\n",
    "    if query_length == kv_seq_len:\n",
    "        query_layer = index_first_axis(\n",
    "            query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n",
    "        )\n",
    "        cu_seqlens_q = cu_seqlens_k\n",
    "        max_seqlen_in_batch_q = max_seqlen_in_batch_k\n",
    "        indices_q = indices_k\n",
    "    elif query_length == 1:\n",
    "        max_seqlen_in_batch_q = 1\n",
    "        cu_seqlens_q = torch.arange(\n",
    "            batch_size + 1, dtype=torch.int32, device=query_layer.device\n",
    "        )  # There is a memcpy here, that is very bad.\n",
    "        indices_q = cu_seqlens_q[:-1]\n",
    "        query_layer = query_layer.squeeze(1)\n",
    "    else:\n",
    "        # The -q_len: slice assumes left padding.\n",
    "        attention_mask = attention_mask[:, -query_length:]\n",
    "        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n",
    "\n",
    "    return (\n",
    "        query_layer,\n",
    "        key_layer,\n",
    "        value_layer,\n",
    "        indices_q,\n",
    "        (cu_seqlens_q, cu_seqlens_k),\n",
    "        (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af09ee-280d-4e8d-a389-88f647f9ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input = _upad_input\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16, use_flash_attention_2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928dc43c-fe80-43e4-b8fc-e73ac4e71efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_concat_samples(batch_data, eos_token_id, reset_position_ids=False):\n",
    "    '''\n",
    "    change attention mask\n",
    "    '''\n",
    "    input_ids = batch_data['input_ids']\n",
    "    labels = batch_data['labels'].clone()\n",
    "    micro_batch_size, seq_length = input_ids.shape\n",
    "\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "    inner_sample_lengths = torch.zeros((micro_batch_size, seq_length), dtype=torch.int)\n",
    "    for b in range(micro_batch_size):\n",
    "        # Find indecies where EOD token is.\n",
    "        eod_index = position_ids[b, input_ids[b] == eos_token_id]\n",
    "        # Detach indecies from positions if going to modify positions.\n",
    "        if reset_position_ids:\n",
    "            eod_index = eod_index.clone()\n",
    "\n",
    "        prev_index = -1\n",
    "        for j in range(len(eod_index)):\n",
    "            inner_sample_lengths[b, j] = eod_index[j] - prev_index\n",
    "            prev_index = eod_index[j]\n",
    "            if eod_index[j] < seq_length - 1:\n",
    "                labels[b, eod_index[j]+1] = -100\n",
    "\n",
    "        if prev_index < seq_length - 1:\n",
    "            inner_sample_lengths[b, len(eod_index)] = seq_length - 1 - prev_index\n",
    "\n",
    "        #print(len(input_ids[b]), sum(inner_sample_lengths[b]))\n",
    "        assert len(input_ids[b]) == sum(inner_sample_lengths[b]).item()\n",
    "\n",
    "        if reset_position_ids and len(eod_index) > 1:\n",
    "            for j in range(1, len(eod_index)):\n",
    "                i = eod_index[j]\n",
    "                prev_len = eod_index[j-1]\n",
    "                position_ids[b, i:] -= (i - prev_len)\n",
    "\n",
    "    batch_data['labels'] = labels\n",
    "    batch_data['attention_mask'] = inner_sample_lengths\n",
    "\n",
    "    if reset_position_ids:\n",
    "        batch_data['position_ids'] = position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2d1a4-adac-4ddf-81a4-1e3064361735",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I guess it would be better to keep position increasing, since most of samples are short.\n",
    "mask_concat_samples(batch, eos_token_id, reset_position_ids=False)\n",
    "\n",
    "outputs = model(**batch, use_cache=False)\n",
    "loss = outputs.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
